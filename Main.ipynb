{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T16:56:08.088145Z",
     "start_time": "2020-12-20T16:56:06.658898Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_augmentation import AugmentData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning on Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T16:57:00.589366Z",
     "start_time": "2020-12-20T16:56:59.875501Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "train = np.load('data/train.npz')\n",
    "X, y = train['arr_0'], train['arr_1']\n",
    "classes = ['ant', 'spider', 'flower', 'dolphin', 'lobster', 'bulldozer']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train['arr_0'], train['arr_1'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-20T16:57:12.265Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/conda/pyanni/envs/kaggle_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 9 is smaller than n_iter=100. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=60)]: Using backend LokyBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done  18 out of  45 | elapsed:  2.1min remaining:  3.2min\n",
      "[Parallel(n_jobs=60)]: Done  41 out of  45 | elapsed:  2.5min remaining:   14.5s\n",
      "[Parallel(n_jobs=60)]: Done  45 out of  45 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.534 \n",
      "best parameters: {'polynomials__degree': 2, 'pca__n_components': 600, 'logistic__penalty': 'l2', 'logistic__C': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/conda/pyanni/envs/kaggle_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipe = Pipeline([('transformer', FunctionTransformer(lambda x: x / 255)),\n",
    "                 ('polynomials', PolynomialFeatures(include_bias=False)),\n",
    "                 ('pca', PCA()),\n",
    "                 ('logistic', LogisticRegression())])\n",
    "\n",
    "parameters = {'polynomials__degree': [2],\n",
    "              'pca__n_components': [600, 650, 700],\n",
    "              'logistic__penalty': ['l2'],\n",
    "              'logistic__C': [0.001, 0.005, 0.01]}\n",
    "\n",
    "clf = RandomizedSearchCV(pipe, parameters, n_iter=100, n_jobs=60, verbose=2,\n",
    "                         random_state=12)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'accuracy: {clf.best_score_:.3f}',\n",
    "      f'\\nbest parameters: { clf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on Augmented Dataset\n",
    "Rotations do not seem to increase accuracy (they actually decrease it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = AugmentData(X_train, y_train, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_train_aug, y_train_aug = aug_data.return_data()\n",
    "\n",
    "fitted_pipe = Pipeline([('transformer', FunctionTransformer(lambda x: x / 255)),\n",
    "                        ('polynomials', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                        ('pca', PCA(n_components=650)),\n",
    "                        ('logistic', LogisticRegression(C=0.005, n_jobs=100))])\n",
    "\n",
    "fitted_pipe.fit(X_train_aug, y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-20T17:00:35.738Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_confusion_matrix(y_test, y_pred, classes):\n",
    "    df_report = pd.DataFrame(confusion_matrix(y_test, y_pred, normalize='true'), columns=classes).round(3)\n",
    "    df_report.index = classes\n",
    "    display(df_report)\n",
    "\n",
    "y_pred = fitted_pipe.predict(X_test)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred):.3f}')\n",
    "          \n",
    "show_confusion_matrix(y_test, y_pred, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = np.load('data/train.npz')\n",
    "X, y = train['arr_0'], train['arr_1']\n",
    "classes = ['ant', 'spider', 'flower', 'dolphin', 'lobster', 'bulldozer']\n",
    "\n",
    "aug_data = AugmentData(X, y, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_aug, y_aug = aug_data.return_data()\n",
    "\n",
    "fitted_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "no submission was made to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test = np.load('data/test.npz')\n",
    "X_test = test['arr_0']\n",
    "y_pred_test = fitted_pipe.predict(X_test)\n",
    "submission = pd.DataFrame({'Id': range(len(y_pred_test)), 'Category': y_pred_test})\n",
    "submission.to_csv('submissions/logistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning on Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "train = np.load('data/train.npz')\n",
    "X, y = train['arr_0'], train['arr_1']\n",
    "classes = ['ant', 'spider', 'flower', 'dolphin', 'lobster', 'bulldozer']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train['arr_0'], train['arr_1'], test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 100)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(2, 120, num = 30)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = range(2, 20)\n",
    "min_samples_leaf = range(2, 20)\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "clf = RandomizedSearchCV(rf, random_grid, n_iter=500, verbose=2, random_state=0, \n",
    "                         n_jobs=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'accuracy: {clf.best_score_:.3f}',\n",
    "      f'\\nbest parameters: {clf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on Augmented Dataset\n",
    "Rotations do not seem to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = AugmentData(X_train, y_train, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_train_aug, y_train_aug = aug_data.return_data()\n",
    "\n",
    "fitted_rf = RandomForestClassifier(**clf.best_params_, n_jobs=100)\n",
    "fitted_rf.fit(X_train_aug, y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fitted_rf.predict(X_test)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred):.3f}')\n",
    "          \n",
    "show_confusion_matrix(y_test, y_pred, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = np.load('data/train.npz')\n",
    "X, y = train['arr_0'], train['arr_1']\n",
    "\n",
    "aug_data = AugmentData(X_train, y_train, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_aug, y_aug = aug_data.return_data()\n",
    "\n",
    "fitted_rf = RandomForestClassifier(**clf.best_params_, n_jobs=100)\n",
    "fitted_rf.fit(X_aug, y_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "test = np.load('data/test.npz')\n",
    "X_test = test['arr_0']\n",
    "y_pred_test = fitted_rf.predict(X_test)\n",
    "submission = pd.DataFrame({'Id': range(len(y_pred_test)), 'Category': y_pred_test})\n",
    "submission.to_csv('submissions/rf1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier (SVC) with Histogram of Oriented Gradients (HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning on Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = np.load('data/train.npz')\n",
    "X, y = train['arr_0'], train['arr_1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train['arr_0'], train['arr_1'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "     \n",
    "parameters = {'C': np.logspace(-4, 0, 5)}\n",
    "\n",
    "clf = RandomizedSearchCV(LinearSVC(), parameters, n_iter=100, n_jobs=60, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'accuracy: {clf.best_score_:.3f}',\n",
    "      f'\\nbest parameters: { clf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "     \n",
    "parameters = {'C': np.linspace(0, 5, 10)}\n",
    "\n",
    "clf = RandomizedSearchCV(SVC(), parameters, n_iter=100, n_jobs=60, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'accuracy: {clf.best_score_:.3f}',\n",
    "      f'\\nbest parameters: { clf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC with HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "\n",
    "def transform_hog(X):\n",
    "    return np.array([hog(resize(x.reshape(28, 28), (32, 32))) for x in X])\n",
    "    \n",
    "pipe = Pipeline(\n",
    "    [('transformer', FunctionTransformer(transform_hog)),\n",
    "     ('SVC', SVC())])\n",
    "\n",
    "parameters = {'SVC__kernel': ['rbf', 'linear'],\n",
    "              'SVC__C': np.linspace(0, 2, 10)}\n",
    "\n",
    "clf = RandomizedSearchCV(pipe, parameters, n_iter=100, n_jobs=60, verbose=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'accuracy: {clf.best_score_:.3f}',\n",
    "      f'\\nbest parameters: { clf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on Augmented Dataset\n",
    "Rotations do not seem to increase accuracy (they actually decrease it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = AugmentData(X_train, y_train, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_train_aug, y_train_aug = aug_data.return_data()\n",
    "\n",
    "fitted_pipe = Pipeline(\n",
    "    [('transformer', FunctionTransformer(transform_hog)),\n",
    "     ('SVC', SVC(C=1.77, kernel='rbf'))])\n",
    "\n",
    "fitted_pipe.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = fitted_pipe.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fitted_pipe.predict(X_test)\n",
    "print(f'accuracy: {accuracy_score(y_test, y_pred):.3f}')\n",
    "          \n",
    "show_confusion_matrix(y_test, y_pred, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = np.load('data/train.npz')\n",
    "X, y = train['arr_0'], train['arr_1']\n",
    "\n",
    "aug_data = AugmentData(X, y_train, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_aug, y_aug = aug_data.return_data()\n",
    "\n",
    "fitted_pipe.fit(X_train_aug, y_train_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test = np.load('data/test.npz')\n",
    "X_test = test['arr_0']\n",
    "y_pred_test = fitted_pipe.predict(X_test)\n",
    "submission = pd.DataFrame({'Id': range(len(y_pred_test)), 'Category': y_pred_test})\n",
    "submission.to_csv('submissions/svm_hog.csv', index=False)"
   ]
  },
  {
   "source": [
    "# CNN with augmented dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from data_augmentation import AugmentData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfile = \"data/train.npz\"\n",
    "images = np.load(dfile)[\"arr_0\"]\n",
    "labels = np.load(dfile)[\"arr_1\"]\n",
    "classes = ['ant', 'spider', 'flower', 'dolphin', 'lobster', 'bulldozer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment dataset\n",
    "aug_data = AugmentData(images, labels, classes)\n",
    "aug_data.add_shifted_images([1, 2, 3])\n",
    "aug_data.add_flipped_images()\n",
    "X_train_aug, y_train_aug = aug_data.return_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.layers.experimental.preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Submission .84 with this model: b=100 x 200, b= 600 x 200\n",
    "model = keras.Sequential([\n",
    "    \n",
    "    # Preprocessing to add random variations to the training data\n",
    "    preprocessing.RandomFlip('horizontal'), # flip left-to-right\n",
    "    preprocessing.RandomContrast(0.5), # contrast change by up to 50%\n",
    "    preprocessing.Normalization(),\n",
    "    preprocessing.RandomZoom(height_factor=(-0.2,0.2), width_factor=(-0.2,0.2)),\n",
    "    preprocessing.RandomRotation(factor=0.05),\n",
    "    preprocessing.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "\n",
    "    # based on this model: https://github.com/ck090/Google_Quick_Draw/blob/master/Myquickdraw.ipynb\n",
    "    # These values were chosen empirically. We found keeping a small number of filters with an extra layer lowered overfitting and gave us better validation accuracy, although further improvements could still be made.\n",
    "    layers.Conv2D(32, (3, 3), input_shape=(28, 28,1), activation='relu'), \n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(3,3)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(6, activation='softmax')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# format the data before training\n",
    "y_train_aug_cnn = np_utils.to_categorical(y_train_aug)\n",
    "X_train_aug_cnn = X_train_aug.reshape(X_train_aug.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "history = model.fit(X_train_aug_cnn, y_train_aug_cnn, batch_size=100, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "\n",
    "test = np.load('data/test.npz')\n",
    "test_images = test['arr_0']\n",
    "X_test_cnn = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "y_preds = model.predict(X_test_cnn)\n",
    "y_cats = np.argmax(y_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some csv\n",
    "\n",
    "import csv\n",
    "with open('pred_file.csv', mode='w') as file:\n",
    "    wr = csv.writer(file, delimiter=',')\n",
    "    i = 0\n",
    "    wr.writerow(['Id','Category'])\n",
    "    for cat in y_cats:\n",
    "      wr.writerow([i,cat])\n",
    "      i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "67d67e971347367d4cd33a621008df16d3827f7dd0d4a165448976b348dc0cf7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}